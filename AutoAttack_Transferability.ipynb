{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003a61cb",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook tests the transferability of AutoAttack [1] on clean samples from the ImageNet-Val [3] dataset. The attacks are performed on a ResNet-50 [4] victim classifier, and the transferability to ResNet-18, ResNet-34, ResNet-152, Inception-V3 [5], and ViT [6] classifiers is calculated. To run the notebook ensure that the environment has been loaded from the provided `environment.yml` file, and that the ImageNet dataset has been downloaded from [here](https://image-net.org/download.php) and extracted to the local file system.\n",
    "\n",
    "We now give an exact definition of the attack success rate (ASR). Given a set of clean samples, $\\{ \\boldsymbol{x}_1 , \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_N \\}$, the corresponding adversarial samples, $\\{ \\tilde{\\boldsymbol{x}}_1 , \\tilde{\\boldsymbol{x}}_2, \\dots, \\tilde{\\boldsymbol{x}}_N \\}$, the associated true labels, $\\{ y_1, y_2, \\dots, y_N \\}$, and--in the case of targeted attacks--the target adversarial labels, $\\{ \\tilde{y}_1, \\tilde{y}_2, \\dots, \\tilde{y}_N \\}$, we define the targeted and untargeted attack success rate (ASR) against classifier a, $f : X \\rightarrow \\mathcal{Y}$, as: $$\\begin{align*}\n",
    "    \\text{targeted ASR} &= \\frac{\\sum_{i=1}^N \\mathbb{1}_{\\{ f(x_i) = y_i \\}} \\cdot \\mathbb{1}_{\\{ f(\\tilde{x}_i) = \\tilde{y}_i \\}}}{\\sum_{i=1}^N \\mathbb{1}_{\\{ f(x_i) = y_i \\}}}; \\\\\n",
    "    \\text{untargeted ASR} &= \\frac{\\sum_{i=1}^N \\mathbb{1}_{\\{ f(x_i) = y_i \\}} \\cdot \\mathbb{1}_{\\{ f(\\tilde{x}_i) \\neq y_i \\}}}{\\sum_{i=1}^N \\mathbb{1}_{\\{ f(x_i) = y_i \\}}}. \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "# References\n",
    "[1] F. Croce and M. Hein, Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks, Auto-Attack, 2020.</br>\n",
    "[2] H. Kim, “Torchattacks: A pytorch repository for adversarial attacks,” arXiv preprint arXiv:2010.01950, 2020.</br>\n",
    "[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and F. F. Li, “ImageNet: A large-scale hierarchical image database,” in 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009.</br>\n",
    "[4] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, ResNet architecture, 2015.</br>\n",
    "[5] C. Szegedy, W. Liu, Y. Jia, et al., Going deeper with convolutions, 2014.</br>\n",
    "[6] A. Dosovitskiy, L. Beyer, A. Kolesnikov, et al., An image is worth 16x16 words: Transformers for image recognition at scale, 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4004f2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d444d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import Compose, Resize\n",
    "\n",
    "from torchvision.models import (\n",
    "    resnet152,\n",
    "    resnet50,\n",
    "    resnet34,\n",
    "    resnet18,\n",
    "    inception_v3,\n",
    "    vit_h_14,\n",
    "    ResNet152_Weights,\n",
    "    ResNet50_Weights,\n",
    "    ResNet34_Weights,\n",
    "    ResNet18_Weights,\n",
    "    Inception_V3_Weights,\n",
    "    ViT_H_14_Weights,\n",
    ")\n",
    "from dataset_readers import ImageDataset, NatAdvDiffImageDataset, ImageNetDataset\n",
    "\n",
    "from typing import Union, Iterable, Optional, Tuple, List, Any, Callable, Dict\n",
    "\n",
    "from autoattack.autoattack import AutoAttack\n",
    "\n",
    "from dataset_readers import IMAGENET_CLASSES\n",
    "from misc import array_to_PIL\n",
    "\n",
    "### SPECIFY DTYPE AND IMAGENET PATH ###\n",
    "PATH_TO_IMAGENET = \"/home/max/DATA/SSD/DATASETS/IMAGENET_LARGE\"\n",
    "DTYPE = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83fe837",
   "metadata": {},
   "source": [
    "## Loading Pytorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_18 = resnet18(ResNet18_Weights.IMAGENET1K_V1).eval().to(dtype=DTYPE, device=\"cuda\")\n",
    "resnet_34 = resnet34(ResNet34_Weights.IMAGENET1K_V1).eval().to(dtype=DTYPE, device=\"cuda\")\n",
    "resnet_50 = resnet50(ResNet50_Weights.IMAGENET1K_V2).eval().to(dtype=DTYPE, device=\"cuda\")\n",
    "resnet_152 = resnet152(ResNet152_Weights.IMAGENET1K_V2).eval().to(dtype=DTYPE, device=\"cuda\")\n",
    "inception = inception_v3(Inception_V3_Weights.IMAGENET1K_V1).eval().to(dtype=DTYPE, device=\"cuda\")\n",
    "vit = vit_h_14(ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1).eval().to(dtype=DTYPE, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718033d",
   "metadata": {},
   "source": [
    "## Model Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efada2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMS = [Resize((224, 224))]\n",
    "preprocessor = Compose(TRANSFORMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151d3709",
   "metadata": {},
   "source": [
    "## Loading ImageNet-Val Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d183754",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_val = ImageNetDataset(image_dir=\"/home/max/DATA/SSD/DATASETS/IMAGENET_LARGE\", imagenet_type=\"val\")\n",
    "print(IMAGENET_CLASSES[\"id2label\"][imagenet_val[0][1].item()])\n",
    "array_to_PIL(imagenet_val[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a163a39b",
   "metadata": {},
   "source": [
    "## Classifier Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_prob(model, img):\n",
    "    prob = torch.softmax(model(img), dim = 1)\n",
    "    predicted_class = prob.argmax(dim = -1).item()\n",
    "    return predicted_class, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15baf8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 9\n",
    "\n",
    "pt_img, y = imagenet_val[idx]\n",
    "pt_img = pt_img.to(dtype = DTYPE, device = \"cuda\").unsqueeze(0)\n",
    "y = y.item()\n",
    "pt_img = pt_img / 255 # Normalising\n",
    "pt_img_processed = preprocessor(pt_img)\n",
    "\n",
    "yres_18, yres_18_prob = get_class_prob(resnet_18, pt_img_processed)\n",
    "yres_34, yres_34_prob = get_class_prob(resnet_34, pt_img_processed)\n",
    "yres_50, yres_50_prob = get_class_prob(resnet_50, pt_img_processed)\n",
    "yres_152, yres_152_prob = get_class_prob(resnet_152, pt_img_processed)\n",
    "yres_inc, yres_inc_prob = get_class_prob(inception, pt_img_processed)\n",
    "yres_vit, yres_vit_prob = get_class_prob(vit, pt_img_processed)\n",
    "\n",
    "print(f\"TRUE CLASS: {IMAGENET_CLASSES['id2label'][y]} ({y})\")\n",
    "print(60*\"=\")\n",
    "print(f\"resnet18 PREDICTION  : {IMAGENET_CLASSES['id2label'][yres_18]} ({round(yres_18_prob[0, yres_18].item()*100,2)}%)\")\n",
    "print(f\"resnet34 PREDICTION  : {IMAGENET_CLASSES['id2label'][yres_34]} ({round(yres_34_prob[0, yres_34].item()*100,2)}%)\")\n",
    "print(f\"resnet50 PREDICTION  : {IMAGENET_CLASSES['id2label'][yres_50]} ({round(yres_50_prob[0, yres_50].item()*100,2)}%)\")\n",
    "print(f\"resnet152 PREDICTION : {IMAGENET_CLASSES['id2label'][yres_152]} ({round(yres_152_prob[0, yres_152].item()*100,2)}%)\")\n",
    "print(f\"inception PREDICTION : {IMAGENET_CLASSES['id2label'][yres_inc]} ({round(yres_inc_prob[0, yres_inc].item()*100,2)}%)\")\n",
    "print(f\"vit PREDICTION       : {IMAGENET_CLASSES['id2label'][yres_vit]} ({round(yres_vit_prob[0, yres_vit].item()*100,2)}%)\")\n",
    "print(60*\"=\")\n",
    "\n",
    "array_to_PIL(pt_img[0].cpu() * 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9160cce",
   "metadata": {},
   "source": [
    "## AutoAttack Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 9\n",
    "MODEL = resnet_50\n",
    "\n",
    "attacker = AutoAttack(MODEL, eps = 8 / 255, norm=\"Linf\")\n",
    "\n",
    "pt_img, y = imagenet_val[idx]\n",
    "pt_img = pt_img.to(dtype = DTYPE, device = \"cuda\").unsqueeze(0)\n",
    "y = y.to(dtype = torch.int64, device = \"cuda\").unsqueeze(0)\n",
    "\n",
    "pt_img = pt_img / 255 # Normalising\n",
    "pt_img_processed = preprocessor(pt_img)\n",
    "\n",
    "pt_adv = attacker.run_standard_evaluation(pt_img_processed, y, bs = 1)\n",
    "\n",
    "y = y.item()\n",
    "\n",
    "yres_18, yres_18_prob = get_class_prob(resnet_18, pt_adv)\n",
    "yres_34, yres_34_prob = get_class_prob(resnet_34, pt_adv)\n",
    "yres_50, yres_50_prob = get_class_prob(resnet_50, pt_adv)\n",
    "yres_152, yres_152_prob = get_class_prob(resnet_152, pt_adv)\n",
    "yres_inc, yres_inc_prob = get_class_prob(inception, pt_adv)\n",
    "yres_vit, yres_vit_prob = get_class_prob(vit, pt_adv)\n",
    "\n",
    "print(f\"TRUE CLASS: {IMAGENET_CLASSES['id2label'][y]} ({y})\")\n",
    "print(60*\"=\")\n",
    "print(f\"resnet18 PREDICTION  : {IMAGENET_CLASSES['id2label'][yres_18]} ({round(yres_18_prob[0, yres_18].item()*100,2)}%)\")\n",
    "print(f\"resnet34 PREDICTION  : {IMAGENET_CLASSES['id2label'][yres_34]} ({round(yres_34_prob[0, yres_34].item()*100,2)}%)\")\n",
    "print(f\"resnet50 PREDICTION  : {IMAGENET_CLASSES['id2label'][yres_50]} ({round(yres_50_prob[0, yres_50].item()*100,2)}%)\")\n",
    "print(f\"resnet152 PREDICTION : {IMAGENET_CLASSES['id2label'][yres_152]} ({round(yres_152_prob[0, yres_152].item()*100,2)}%)\")\n",
    "print(f\"inception PREDICTION : {IMAGENET_CLASSES['id2label'][yres_inc]} ({round(yres_inc_prob[0, yres_inc].item()*100,2)}%)\")\n",
    "print(f\"vit PREDICTION       : {IMAGENET_CLASSES['id2label'][yres_vit]} ({round(yres_vit_prob[0, yres_vit].item()*100,2)}%)\")\n",
    "print(60*\"=\")\n",
    "\n",
    "array_to_PIL(pt_adv[0].cpu() * 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181bc4db",
   "metadata": {},
   "source": [
    "## Running AutoAttack on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a50e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50 # Number of ImageNet images to attack\n",
    "MODEL = resnet_50 # Model to generate the adversarial samples with\n",
    "\n",
    "attacker = AutoAttack(MODEL, eps = 4 / 255, norm=\"Linf\", verbose=False)\n",
    "\n",
    "clean_accuracies = {\"res18\" : 0, \"res34\" : 0, \"res50\" : 0, \"res152\" : 0, \"inc\" : 0, \"vit\" : 0}\n",
    "adversarial_accuracies = {\"res18\" : 0, \"res34\" : 0, \"res50\" : 0, \"res152\" : 0, \"inc\" : 0, \"vit\" : 0}\n",
    "untargeted_attack_success = {\"res18\" : 0, \"res34\" : 0, \"res50\" : 0, \"res152\" : 0, \"inc\" : 0, \"vit\" : 0}\n",
    "overlap_totals = {\"res18\" : 0, \"res34\" : 0, \"res50\" : 0, \"res152\" : 0, \"inc\" : 0, \"vit\" : 0}\n",
    "for i in tqdm(range(N)):\n",
    "    pt_img, y = imagenet_val[i]\n",
    "    pt_img = pt_img.to(dtype = DTYPE, device = \"cuda\").unsqueeze(0)\n",
    "    y = y.to(dtype = torch.int64, device = \"cuda\").unsqueeze(0)\n",
    "\n",
    "    pt_img = pt_img / 255 # Normalising\n",
    "    pt_img_processed = preprocessor(pt_img)\n",
    "\n",
    "    adv_results = attacker.run_standard_evaluation(pt_img_processed, y, return_labels = True, bs = 1)\n",
    "    pt_adv, ya = adv_results\n",
    "\n",
    "    y = y.item()\n",
    "\n",
    "    # Clean\n",
    "    yres_18, yres_18_prob = get_class_prob(resnet_18, pt_img_processed)\n",
    "    yres_34, yres_34_prob = get_class_prob(resnet_34, pt_img_processed)\n",
    "    yres_50, yres_50_prob = get_class_prob(resnet_50, pt_img_processed)\n",
    "    yres_152, yres_152_prob = get_class_prob(resnet_152, pt_img_processed)\n",
    "    y_inc, y_inc_prob = get_class_prob(inception, pt_img_processed)\n",
    "    y_vit, y_vit_prob = get_class_prob(vit, pt_img_processed)\n",
    "\n",
    "    # Adversarial\n",
    "    ayres_18, ayres_18_prob = get_class_prob(resnet_18, pt_adv)\n",
    "    ayres_34, ayres_34_prob = get_class_prob(resnet_34, pt_adv)\n",
    "    ayres_50, ayres_50_prob = get_class_prob(resnet_50, pt_adv)\n",
    "    ayres_152, ayres_152_prob = get_class_prob(resnet_152, pt_adv)\n",
    "    ay_inc, ay_inc_prob = get_class_prob(inception, pt_adv)\n",
    "    ay_vit, ay_vit_prob = get_class_prob(vit, pt_adv)\n",
    "\n",
    "    # Clean Accuracy\n",
    "    if yres_18 == y:\n",
    "        clean_accuracies[\"res18\"] += 1\n",
    "    if yres_34 == y:\n",
    "        clean_accuracies[\"res34\"] += 1\n",
    "    if yres_50 == y:\n",
    "        clean_accuracies[\"res50\"] += 1\n",
    "    if yres_152 == y:\n",
    "        clean_accuracies[\"res152\"] += 1\n",
    "    if y_inc == y:\n",
    "        clean_accuracies[\"inc\"] += 1\n",
    "    if y_vit == y:\n",
    "        clean_accuracies[\"vit\"] += 1\n",
    "\n",
    "    # Adversarial Accuracy\n",
    "    if ayres_18 == y:\n",
    "        adversarial_accuracies[\"res18\"] += 1\n",
    "    if ayres_34 == y:\n",
    "        adversarial_accuracies[\"res34\"] += 1\n",
    "    if ayres_50 == y:\n",
    "        adversarial_accuracies[\"res50\"] += 1\n",
    "    if ayres_152 == y:\n",
    "        adversarial_accuracies[\"res152\"] += 1\n",
    "    if ay_inc == y:\n",
    "        adversarial_accuracies[\"inc\"] += 1\n",
    "    if ay_vit == y:\n",
    "        adversarial_accuracies[\"vit\"] += 1\n",
    "\n",
    "    # Attack Success Rate\n",
    "    mpred, _ = get_class_prob(MODEL, pt_img_processed)\n",
    "    if yres_18 == mpred == y:\n",
    "        if ayres_18 != y:\n",
    "            untargeted_attack_success[\"res18\"] += 1\n",
    "        overlap_totals[\"res18\"] += 1 \n",
    "    if yres_34 == mpred == y:\n",
    "        if ayres_34 != y:\n",
    "            untargeted_attack_success[\"res34\"] += 1\n",
    "        overlap_totals[\"res34\"] += 1\n",
    "    if yres_50 == mpred == y:\n",
    "        if ayres_50 != y:\n",
    "            untargeted_attack_success[\"res50\"] += 1\n",
    "        overlap_totals[\"res50\"] += 1\n",
    "    if yres_152 == mpred == y:\n",
    "        if ayres_152 != y:\n",
    "            untargeted_attack_success[\"res152\"] += 1\n",
    "        overlap_totals[\"res152\"] += 1\n",
    "    if y_inc == mpred == y:\n",
    "        if ay_inc != y:\n",
    "            untargeted_attack_success[\"inc\"] += 1\n",
    "        overlap_totals[\"inc\"] += 1\n",
    "    if y_vit == mpred == y:\n",
    "        if ay_vit != y:\n",
    "            untargeted_attack_success[\"vit\"] += 1\n",
    "        overlap_totals[\"vit\"] += 1\n",
    "\n",
    "for k, v in clean_accuracies.items():\n",
    "    clean_accuracies[k] = v / N * 100\n",
    "for k, v in adversarial_accuracies.items():\n",
    "    adversarial_accuracies[k] = v / N * 100\n",
    "for k, v in untargeted_attack_success.items():\n",
    "    untargeted_attack_success[k] = v / overlap_totals[k] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6732ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = {\n",
    "    \"Clean Accuracy\" : clean_accuracies,\n",
    "    \"Adversarial Accuracy\" : adversarial_accuracies,\n",
    "    \"Untargeted Attack Success Rate\" : untargeted_attack_success\n",
    "}\n",
    "pd.DataFrame(DATA).T.round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoAttackTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
